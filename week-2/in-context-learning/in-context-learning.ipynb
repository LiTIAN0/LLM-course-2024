{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDSSSkaaDHQ"
      },
      "source": [
        "# In-Context Learning\n",
        "\n",
        "\n",
        "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
        "\n",
        "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
        "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
        "\n",
        "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
        "* a search engine is used to retrieve some relevant information\n",
        "* that information is then provided to the LLM as context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7dVVH85aDHR"
      },
      "source": [
        "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vjT6123RaDHS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from IPython.display import Markdown, display\n",
        "from pypdf import PdfReader\n",
        "from datetime import date\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ixOGTDJ-aDHT"
      },
      "outputs": [],
      "source": [
        "API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWktXpbZaDHT"
      },
      "source": [
        "We select those papers that have been featured in Hugging Face papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kireq1dYaDHT"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://huggingface.co/papers\"\n",
        "page = requests.get(BASE_URL)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "h3s = soup.find_all(\"h3\")\n",
        "\n",
        "papers = []\n",
        "\n",
        "for h3 in h3s:\n",
        "    a = h3.find(\"a\")\n",
        "    title = a.text\n",
        "    link = a[\"href\"].replace('/papers', '')\n",
        "\n",
        "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HImBOlCfaDHT"
      },
      "source": [
        "Code to extract text from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nREjMfs6aDHU"
      },
      "outputs": [],
      "source": [
        "def extract_paper(url):\n",
        "    html = urlopen(url).read()\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_pdf(url):\n",
        "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
        "    reader = PdfReader(\"pdf_file.pdf\")\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "18EVEyyIaDHU"
      },
      "outputs": [],
      "source": [
        "LLM = \"gemini-1.5-flash\"\n",
        "model = genai.GenerativeModel(LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_He2wZSaaDHU"
      },
      "source": [
        "We use Gemini to summarize the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OPOc0zVRaDHU",
        "outputId": "f017cda1-846b-4b6a-9320-f84036c4110b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:23<00:00,  5.89s/it]\n"
          ]
        }
      ],
      "source": [
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. \" + extract_pdf(paper[\"url\"])).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"summary\"] = \"Paper not available\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kHeWIpBaDHV"
      },
      "source": [
        "We print the results to a html file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SFMaW_aeaDHV"
      },
      "outputs": [],
      "source": [
        "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
        "with open(\"papers.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "for paper in papers:\n",
        "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
        "    with open(\"papers.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "end = \"</head>  </html>\"\n",
        "with open(\"papers.html\", \"a\") as f:\n",
        "    f.write(end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_PmBc5naDHV"
      },
      "source": [
        "We can also print the results to this notebook as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "9YECtrV6aDHV",
        "outputId": "1d0fa86f-a30a-4b6f-8c72-4ea49f8e6d44"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**<br>This research paper introduces YuLan-Mini, a 2.42B parameter language model achieving top-tier performance among similarly sized models.  Its strengths lie in a data-efficient pre-training approach encompassing three key contributions: a refined data pipeline combining cleaning and scheduling strategies, a robust optimization method to mitigate training instability (using techniques like µP initialization and WeSaR re-parameterization), and an effective annealing approach incorporating targeted data selection and long-context training.  Remarkably, YuLan-Mini achieves performance comparable to larger models trained on substantially more data.  However, a weakness is the limited context length (28K) due to resource constraints, hindering a full comparison with models offering longer contexts.  Another potential weakness is the reliance on a relatively small number of benchmark tests, which might not fully capture the model's capabilities across all tasks.  Despite these limitations, the open-sourcing of the model and training details is a significant strength, facilitating reproducibility within the research community.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**<br>This research paper comprehensively investigates gist token-based context compression methods for enhancing long-context processing in large language models (LLMs).  The study finds that while this approach achieves near-lossless performance on tasks like retrieval-augmented generation and long-document QA, especially with a fine-grained key-value cache architecture, it struggles with tasks requiring precise recall, such as synthetic recall.  The authors identify three key failure patterns stemming from compression bottlenecks: information loss at segment boundaries, preferential retention of contextually relevant information, and gradual information loss during multi-step processes. To address these limitations, they propose two effective strategies: fine-grained autoencoding and segment-wise token importance estimation. While these strategies significantly improve performance, particularly under low compression ratios, the study is limited by the scale of models and the scope of compression methods considered, leaving room for future investigation into larger models and broader comparisons with alternative compression techniques.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**<br>Molar is a novel multimodal large language model (MLLM)-based sequential recommendation framework that addresses limitations of existing LLM approaches by integrating collaborative filtering.  Its strength lies in using an MLLM to generate unified item representations from textual and non-textual data, followed by a post-alignment mechanism that aligns user representations from content-based and ID-based models, improving personalization and robustness.  Experiments show significant performance gains over traditional and other LLM-based methods across multiple datasets. However, a weakness is the computationally intensive multi-task fine-tuning of the MLLM, potentially hindering real-time applications.  Further limitations include reliance on the underlying MLLM's capabilities; suboptimal base models could negatively impact performance.  Future work aims to address these limitations through an end-to-end training framework and the use of larger LLMs.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**<br>MMFactory is a novel framework designed as a universal solution search engine for vision-language tasks.  Its strength lies in its ability to generate a diverse pool of programmatic solutions tailored to user-specified tasks, constraints (e.g., computational resources, performance targets), and a few input-output examples.  This is achieved through a multi-agent LLM system that proposes and refines solutions iteratively, leveraging a repository of vision, language, and vision-language models.  MMFactory also incorporates a metric router to benchmark and compare the performance and resource usage of each proposed solution, allowing users to make informed decisions.  However, a weakness is the computational cost of the multi-agent system, particularly as the number of existing solutions grows. While the framework reduces API calls compared to sample-specific solutions, the initial solution generation time is significant. Additionally, the reliance on powerful LLMs (like GPT-4) limits accessibility and the generalizability of the framework to users without access to such resources.\n<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for paper in papers:\n",
        "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
        "                                                paper[\"url\"],\n",
        "                                                paper[\"summary\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified prompt for tabulated analysis\n",
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        prompt = \"\"\"Analyze this research article and provide:\n",
        "1. A brief one-sentence summary\n",
        "2. Key strengths (list 2-3 points)\n",
        "3. Key weaknesses (list 2-3 points)\n",
        "\n",
        "Format the response as follows:\n",
        "Summary: [one sentence]\n",
        "| Strengths | Weaknesses |\n",
        "| --- | --- |\n",
        "| [strength 1] | [weakness 1] |\n",
        "| [strength 2] | [weakness 2] |\n",
        "| [strength 3] | [weakness 3] |\n",
        "\n",
        "Article text: \"\"\" + extract_pdf(paper[\"url\"])\n",
        "\n",
        "        paper[\"analysis\"] = model.generate_content(prompt).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"analysis\"] = \"Paper not available\"\n",
        "\n",
        "# Modified markdown printing\n",
        "for paper in papers:\n",
        "    printmd(f\"\"\"**[{paper['title']}]({paper['url']})**\\n\n",
        "{paper['analysis']}\\n\\n---\\n\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wbimiUyMcVmB",
        "outputId": "404cc1db-ba86-4cc5-c087-a0ba53f71489"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:19<00:00,  4.93s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**\n\nSummary: YuLan-Mini is a data-efficient 2.42B parameter language model achieving top-tier performance among similarly sized models by employing an elaborate data pipeline, robust optimization methods, and an effective annealing approach.\n\n| Strengths | Weaknesses |\n|---|---|\n| Achieves top-tier performance comparable to much larger models with significantly less data, demonstrating high data efficiency. | Limited long context capabilities due to resource constraints; context window only extended to 28K tokens. |\n| Open-source and reproducible:  The authors provide full details of data composition and training processes, facilitating reproduction by the research community. |  Difficulty in fully reproducing baseline model results due to incomplete information provided by some baseline studies, affecting the fairness of the comparison. |\n|  Strong performance across various benchmarks, showcasing its versatility in mathematical reasoning, code generation, and general language understanding tasks. |  |\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**\n\nSummary: This research paper comprehensively investigates gist token-based context compression methods for improving long-context processing in large language models, identifying key failure patterns and proposing effective strategies to mitigate them.\n\n| Strengths | Weaknesses |\n|---|---|\n| Comprehensive evaluation of various gist-based architectures and their performance across diverse tasks. | Limited model scale and context length due to computational resource constraints.  |\n| Identification of three key failure patterns (lost by the boundary, lost if surprise, lost along the way) arising from compression bottlenecks, providing valuable insights. |  Focus solely on gist token-based methods; exclusion of other context compression techniques limits the generalizability of findings. |\n| Proposal of two novel strategies (fine-grained autoencoding and segment-wise token importance estimation) to significantly improve the effectiveness of context compression. |\n\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**\n\nSummary: Molar, a novel multimodal large language model framework, enhances sequential recommendation by integrating multimodal item information with collaborative filtering signals through a post-alignment mechanism, outperforming existing LLM-based and traditional methods.\n\n| Strengths | Weaknesses |\n|---|---|\n| Effectively integrates multimodal (text and image) data with ID-based collaborative filtering information for improved recommendation accuracy. |  Requires multi-task fine-tuning, which is computationally expensive and may hinder real-time applications. |\n|  Consistently outperforms traditional and other LLM-based sequential recommendation models across multiple datasets. | The performance heavily relies on the underlying capabilities of the MLLM used; suboptimal base models can lead to degraded performance. |\n| Employs a post-alignment contrastive learning mechanism to effectively combine content-based and ID-based user embeddings, avoiding the limitations of early fusion. |  Larger LLMs could not be fully trained due to computational constraints, limiting the potential performance gains. |\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**\n\nSummary: MMFactory is a universal framework that acts as a solution search engine for vision-language tasks, proposing multiple programmatic solutions tailored to user specifications and constraints by combining various vision, language, and vision-language models.\n\n| Strengths | Weaknesses |\n|---|---|\n| Proposes multiple solutions with performance and resource cost analysis, allowing users to choose the best fit for their needs. | The framework relies heavily on large language models (LLMs), which can be computationally expensive and may not always be accessible to all users. |\n| Addresses the limitations of existing methods by generating generalizable solutions applicable to all instances of a task, not just individual samples. |  The evaluation of solutions may be biased by the choice of LLMs used for metric selection and performance assessment.  |\n| Uses a multi-agent LLM conversation to create robust and diverse solutions. | The paper lacks a detailed discussion on the limitations and potential biases introduced by using specific LLMs for core functionalities, such as solution and metric routing. |\n\n\n\n---\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified HTML printing\n",
        "page = f\"\"\"<html>\n",
        "<head>\n",
        "    <style>\n",
        "        table {{\n",
        "            border-collapse: collapse;\n",
        "            width: 100%;\n",
        "            margin: 20px 0;\n",
        "        }}\n",
        "        th, td {{\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 8px;\n",
        "            text-align: left;\n",
        "        }}\n",
        "        th {{\n",
        "            background-color: #f2f2f2;\n",
        "        }}\n",
        "    </style>\n",
        "    <h1>Daily Dose of AI Research</h1>\n",
        "    <h4>{date.today()}</h4>\n",
        "    <p><i>Analysis generated with: {LLM}</i></p>\n",
        "</head>\n",
        "<body>\"\"\"\n",
        "\n",
        "with open(\"papers_table.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "\n",
        "\n",
        "for paper in papers:\n",
        "    analysis_html = paper['analysis'].replace('|', '</td><td>').replace('\\n', '</td></tr><tr><td>')\n",
        "    page = f\"\"\"\n",
        "    <h2><a href=\"{paper['url']}\">{paper['title']}</a></h2>\n",
        "    <table>\n",
        "        <tr><td>{analysis_html}</td></tr>\n",
        "    </table>\n",
        "    <hr>\"\"\"\n",
        "    with open(\"papers_table.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "\n",
        "\n",
        "end = \"</body></html>\"\n",
        "with open(\"papers_table.html\", \"a\") as f:\n",
        "    f.write(end)"
      ],
      "metadata": {
        "id": "iTjRNznKdqHn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open source model setup\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\", # Use GPU if available\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500, # Reduced max tokens\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.1, # Increased temperature slightly\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9, # Added top_p sampling\n",
        "}\n",
        "\n",
        "# Modified paper analysis loop for the open source model\n",
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        # Limit the input text length\n",
        "        pdf_text = extract_pdf(paper[\"url\"])\n",
        "        max_input_length = 2000\n",
        "        truncated_text = pdf_text[:max_input_length]\n",
        "\n",
        "        messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a research paper analyzer. Provide analysis in a table format with strengths and weaknesses.\"\n",
        "        }, {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Analyze this research article and provide:\n",
        "1. A brief one-sentence summary\n",
        "2. Key strengths (list 2-3 points)\n",
        "3. Key weaknesses (list 2-3 points)\n",
        "\n",
        "Format the response as follows:\n",
        "Summary: [one sentence]\n",
        "| Strengths | Weaknesses |\n",
        "| --- | --- |\n",
        "| [strength 1] | [weakness 1] |\n",
        "| [strength 2] | [weakness 2] |\n",
        "| [strength 3] | [weakness 3] |\n",
        "\n",
        "Article text: {truncated_text}\"\"\"\n",
        "        }]\n",
        "\n",
        "        paper[\"analysis\"] = pipe(messages, **generation_args)[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Generation failed for {paper['title']}: {e}\")\n",
        "        paper[\"analysis\"] = \"Paper not available\"\n",
        "\n",
        "# Modified markdown printing\n",
        "for paper in papers:\n",
        "    printmd(f\"\"\"**[{paper['title']}]({paper['url']})**\\n\n",
        "{paper['analysis']}\\n\\n---\\n\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1ab4860fbc7f45ed98bfedf5b86ee183",
            "a71f5b0ee1cd42f08d96941faac5019d",
            "bab437cfc7ad43d29e018744f95b9908",
            "45a0a264dc3c4e078e3d5ab7133098c5",
            "8207ada29b4544e2801fc394c690de98",
            "8f1aabdf3b53431c99c89cc416f503d4",
            "b924bf6745b34b65b1404c8d39cd7c33",
            "3991e7ab597f466a865062003e63d66b",
            "4bd2b8b0b25b4052892aa8a117c5ed31",
            "d588dbb57120402a81ef4eeb40d043a6",
            "a4e99a105e7348b49d9d35cd6a0b5394"
          ]
        },
        "id": "vc2Afls-qnB8",
        "outputId": "ee28e79c-fc1f-4ed1-b863-8f06b6f5240d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ab4860fbc7f45ed98bfedf5b86ee183"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "  0%|          | 0/4 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
            "100%|██████████| 4/4 [01:54<00:00, 28.59s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**\n\n Summary: YuLan-Mini is a highly efficient language model with 2.42B parameters that achieves top performance with significantly less data than industry-leading models, thanks to its innovative pre-training approach.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Achieves top-tier performance with a significantly smaller dataset (1.08T tokens) compared to industry standards, demonstrating data efficiency. | 1. The paper may not fully address the potential limitations or challenges in scaling the model beyond the current parameter size or data volume. |\n| 2. Introduces a novel pre-training approach with three key technical contributions: an elaborate data pipeline, a robust optimization method, and an effective annealing approach, which could be beneficial for future research and development in the field. | 2. The paper's detailed technical report may be complex and require a deep understanding of machine learning and language modeling, potentially limiting accessibility for a broader audience. |\n| 3. Facilitates reproducibility and further research by releasing full details of the data composition for each training phase and providing access to project details on GitHub, promoting transparency and collaboration in the AI community. | 3. The paper does not discuss the potential ethical considerations or biases that may arise from using a highly efficient but large-scale language model, which is an important aspect of responsible AI development. |\n\nNote: The weaknesses listed are inferred based on common challenges and considerations in the field of AI research and development. The actual weaknesses may vary depending on the specific context and further analysis of the paper.\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**\n\n Summary: The study investigates gist-based context compression methods to enhance long-context processing in large language models, revealing near-lossless performance in certain tasks but identifying challenges and failure patterns in others, and proposing strategies to mitigate these issues.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Comprehensive investigation of gist-based context compression methods, providing valuable insights into their effectiveness and limitations. | 1. Identified specific tasks (e.g., synthetic recall) where gist-based compression faces challenges, indicating potential limitations in broader applications. |\n| 2. Proposed practical strategies (fine-grained autoencoding and segment-wise token importance estimation) to improve compression capabilities, offering actionable solutions for researchers and practitioners. | 2. The study's findings on failure patterns (lost by the boundary, lost if surprise, lost along the way) suggest that further research may be needed to fully understand and address these issues in different contexts. |\n| 3. Extensive experimental validation across various tasks (retrieval-augmented generation, long-document QA), demonstrating the potential of gist-based compression in enhancing long-context processing in large language models. | 3. The study's focus on specific tasks and failure patterns may limit the generalizability of its findings, suggesting a need for broader exploration of gist-based context compression in diverse scenarios. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**\n\n Summary: Molar is a novel framework for sequential recommendation that integrates multimodal content with collaborative filtering signals, significantly outperforming traditional and LLM-based baselines.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. **Integration of Multimodal Data**: Molar effectively combines textual and non-textual data, enriching item embeddings and capturing a more comprehensive representation of items. | 1. **Complexity and Resource Intensity**: The framework may require significant computational resources and expertise to implement, which could limit its accessibility and scalability. |\n| 2. **Collaborative Filtering Alignment**: The post-alignment mechanism aligns user representations from content-based and ID-based models, ensuring personalized recommendations and robust performance. | 2. **Potential Overfitting**: The sophisticated modeling approach might lead to overfitting, especially if not properly regularized or if the training data is not sufficiently diverse. |\n| 3. **Superior Performance**: Extensive experiments demonstrate that Molar significantly outperforms traditional and LLM-based baselines, indicating its effectiveness in leveraging multimodal data and collaborative signals for sequential recommendation tasks. | 3. **Generalization Across Diverse Domains**: While Molar shows superior performance in tested scenarios, its effectiveness across different domains or with varying types of data (e.g., social media, e-commerce) may not be as pronounced without further adaptation or tuning. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**\n\n Summary: MMFactory is a universal framework that acts as a solution search engine for vision-language tasks, offering a diverse pool of programmatic solutions based on task descriptions, sample inputs/outputs, and user-defined constraints.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Provides a universal framework that can handle a variety of vision-language tasks, reducing the need for specialized models for each task. <br> 2. Offers a user-friendly interface by allowing users to input task descriptions, sample inputs/outputs, and constraints, simplifying the process of finding suitable solutions. <br> 3. Incorporates a committee-based solution proposer that leverages multi-agent LLM conversation, enhancing the generation of diverse, universal, and robust solutions. | 1. The effectiveness of MMFactory heavily relies on the quality and diversity of the models available in its repository, which may limit its performance on tasks requiring highly specialized models. <br> 2. The framework's ability to suggest suitable solutions is contingent on the user's ability to accurately describe the task and provide relevant sample inputs/outputs, which may pose a challenge for users with limited technical knowledge or language proficiency. <br> 3. The proposed metrics and benchmarks for performance and resource characteristics may not fully capture the complexity or nuances of certain tasks, potentially leading to suboptimal solution selection.\n\n---\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ab4860fbc7f45ed98bfedf5b86ee183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a71f5b0ee1cd42f08d96941faac5019d",
              "IPY_MODEL_bab437cfc7ad43d29e018744f95b9908",
              "IPY_MODEL_45a0a264dc3c4e078e3d5ab7133098c5"
            ],
            "layout": "IPY_MODEL_8207ada29b4544e2801fc394c690de98"
          }
        },
        "a71f5b0ee1cd42f08d96941faac5019d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f1aabdf3b53431c99c89cc416f503d4",
            "placeholder": "​",
            "style": "IPY_MODEL_b924bf6745b34b65b1404c8d39cd7c33",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "bab437cfc7ad43d29e018744f95b9908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3991e7ab597f466a865062003e63d66b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4bd2b8b0b25b4052892aa8a117c5ed31",
            "value": 2
          }
        },
        "45a0a264dc3c4e078e3d5ab7133098c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d588dbb57120402a81ef4eeb40d043a6",
            "placeholder": "​",
            "style": "IPY_MODEL_a4e99a105e7348b49d9d35cd6a0b5394",
            "value": " 2/2 [00:29&lt;00:00, 14.05s/it]"
          }
        },
        "8207ada29b4544e2801fc394c690de98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f1aabdf3b53431c99c89cc416f503d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b924bf6745b34b65b1404c8d39cd7c33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3991e7ab597f466a865062003e63d66b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd2b8b0b25b4052892aa8a117c5ed31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d588dbb57120402a81ef4eeb40d043a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4e99a105e7348b49d9d35cd6a0b5394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}