{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDSSSkaaDHQ"
      },
      "source": [
        "# In-Context Learning\n",
        "\n",
        "\n",
        "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
        "\n",
        "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
        "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
        "\n",
        "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
        "* a search engine is used to retrieve some relevant information\n",
        "* that information is then provided to the LLM as context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7dVVH85aDHR"
      },
      "source": [
        "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "vjT6123RaDHS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from IPython.display import Markdown, display\n",
        "from pypdf import PdfReader\n",
        "from datetime import date\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ixOGTDJ-aDHT"
      },
      "outputs": [],
      "source": [
        "#API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
        "API_KEY = \"AIzaSyBSAzkemsULoVmt720vZmzZU3MS0DdTWdY\"\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWktXpbZaDHT"
      },
      "source": [
        "We select those papers that have been featured in Hugging Face papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kireq1dYaDHT"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://huggingface.co/papers\"\n",
        "page = requests.get(BASE_URL)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "h3s = soup.find_all(\"h3\")\n",
        "\n",
        "papers = []\n",
        "\n",
        "for h3 in h3s:\n",
        "    a = h3.find(\"a\")\n",
        "    title = a.text\n",
        "    link = a[\"href\"].replace('/papers', '')\n",
        "\n",
        "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HImBOlCfaDHT"
      },
      "source": [
        "Code to extract text from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nREjMfs6aDHU"
      },
      "outputs": [],
      "source": [
        "def extract_paper(url):\n",
        "    html = urlopen(url).read()\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_pdf(url):\n",
        "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
        "    reader = PdfReader(\"pdf_file.pdf\")\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "18EVEyyIaDHU"
      },
      "outputs": [],
      "source": [
        "LLM = \"gemini-1.5-flash\"\n",
        "model = genai.GenerativeModel(LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_He2wZSaaDHU"
      },
      "source": [
        "We use Gemini to summarize the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OPOc0zVRaDHU",
        "outputId": "a5019d4d-1550-4442-9943-057e5196a0ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:20<00:00,  5.07s/it]\n"
          ]
        }
      ],
      "source": [
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. \" + extract_pdf(paper[\"url\"])).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"summary\"] = \"Paper not available\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kHeWIpBaDHV"
      },
      "source": [
        "We print the results to a html file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SFMaW_aeaDHV"
      },
      "outputs": [],
      "source": [
        "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
        "with open(\"papers.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "for paper in papers:\n",
        "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
        "    with open(\"papers.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "end = \"</head>  </html>\"\n",
        "with open(\"papers.html\", \"a\") as f:\n",
        "    f.write(end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_PmBc5naDHV"
      },
      "source": [
        "We can also print the results to this notebook as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "9YECtrV6aDHV",
        "outputId": "9a73053a-c1af-43bf-ed2b-39bdc701b00b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**<br>This research paper details the development of YuLan-Mini, a 2.42B parameter language model achieving state-of-the-art performance for its size.  Its strengths lie in a meticulously designed data pipeline incorporating data cleaning, scheduling strategies, and synthetic data generation, particularly for reasoning tasks.  A robust optimization method effectively mitigates training instability, even with a large learning rate,  and an annealing approach with targeted data selection and long-context training further enhances performance.  The authors' open-sourcing of the training details and data composition promotes reproducibility. However, a weakness is the relatively limited training data (1.08T tokens) compared to industry models, although still significantly less than those achieving comparable performance.  Further limitations include the relatively short context length achieved (28K tokens) due to resource constraints and the challenge of fully replicating baseline model results due to incomplete public reporting of their methodologies.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**<br>This research paper comprehensively investigates gist token-based context compression methods for improving long-context processing in large language models (LLMs).  The study finds that while these methods achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, they struggle with tasks requiring precise recall, like synthetic recall.  The authors identify three key failure patterns stemming from compression bottlenecks: information loss at segment boundaries, preferential retention of contextually relevant information, and information loss during multi-step reasoning. To address these weaknesses, they propose two effective strategies: fine-grained autoencoding and segment-wise token importance estimation, which significantly improve performance, especially under lower compression ratios. A strength of the study is its thorough experimental evaluation across diverse tasks and its insightful analysis of failure modes. However, a limitation is the scope of compression methods considered, focusing solely on gist token-based approaches and not accounting for the potential impact of using larger model sizes or different training data.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**<br>Molar is a novel multimodal large language model (MLLM) framework for sequential recommendation that addresses the limitations of existing LLM-based approaches by integrating collaborative filtering.  Its strengths lie in its use of an MLLM to generate unified item representations from textual and non-textual data, enriching item embeddings, and a post-alignment mechanism that effectively combines content-based and ID-based user representations for improved personalization.  Experiments show significant performance improvements over traditional and other LLM-based methods across multiple datasets. However, a weakness is the computationally expensive multi-task fine-tuning required for optimal performance, potentially hindering real-time applications.  Furthermore, the reliance on a pre-trained MLLM means that the quality of the recommendations is dependent on the capabilities of that underlying model.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**<br>MMFactory is a novel framework designed as a universal solution search engine for vision-language tasks.  It addresses limitations of existing methods by proposing a diverse pool of programmatic solutions composed of various vision, language, and vision-language models, tailored to user-specified tasks, sample input-output pairs, and resource constraints.  A committee-based solution proposer, leveraging multi-agent LLM conversation, generates executable and robust solutions.  Experimental results demonstrate state-of-the-art performance on benchmark datasets.  However,  a weakness is the computational cost of the multi-agent system, although the framework mitigates this by generating reusable solutions applicable across all task instances, reducing the overall API call cost compared to sample-specific approaches.  Further research could explore optimizing the multi-agent conversation process to reduce runtime while maintaining solution quality.\n<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for paper in papers:\n",
        "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
        "                                                paper[\"url\"],\n",
        "                                                paper[\"summary\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified prompt for tabulated analysis\n",
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        prompt = \"\"\"Analyze this research article and provide:\n",
        "1. A brief one-sentence summary\n",
        "2. Key strengths (list 2-3 points)\n",
        "3. Key weaknesses (list 2-3 points)\n",
        "\n",
        "Format the response as follows:\n",
        "Summary: [one sentence]\n",
        "| Strengths | Weaknesses |\n",
        "| --- | --- |\n",
        "| [strength 1] | [weakness 1] |\n",
        "| [strength 2] | [weakness 2] |\n",
        "| [strength 3] | [weakness 3] |\n",
        "\n",
        "Article text: \"\"\" + extract_pdf(paper[\"url\"])\n",
        "\n",
        "        paper[\"analysis\"] = model.generate_content(prompt).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"analysis\"] = \"Paper not available\"\n",
        "\n",
        "# Modified markdown printing\n",
        "for paper in papers:\n",
        "    printmd(f\"\"\"**[{paper['title']}]({paper['url']})**\\n\n",
        "{paper['analysis']}\\n\\n---\\n\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wbimiUyMcVmB",
        "outputId": "82b702b9-3786-4c70-fd56-d16e99049fd7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:20<00:00,  5.12s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**\n\nSummary: YuLan-Mini is a data-efficient 2.42B parameter language model achieving top-tier performance among similarly sized models by employing an elaborate data pipeline, robust optimization methods, and an effective annealing approach.\n\n| Strengths | Weaknesses |\n|---|---|\n| Achieves top-tier performance comparable to much larger models with significantly less training data (data efficiency). | Limited long-context capabilities due to resource constraints on long context training. |\n| Openly releases full training details and data composition to facilitate reproducibility. |  Lack of detailed comparison with baseline models due to missing information on baseline evaluation setups. |\n| Employs multiple innovative techniques to improve training stability and efficiency, such as a combined µP-like initialization with WeSaR re-parameterization, and fused kernels for faster training. |\n\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**\n\nSummary: This research comprehensively investigates gist token-based context compression for long-context processing in large language models, identifying key failure patterns and proposing strategies to mitigate them.\n\n| Strengths | Weaknesses |\n|---|---|\n| Thorough investigation and comprehensive evaluation of gist token-based context compression across various tasks and architectures. | Limited model scale and context length in experiments, restricting generalizability to larger models and longer contexts. |\n| Identification of three critical failure patterns (lost by the boundary, lost if surprise, and lost along the way) providing valuable insights into the limitations of current methods. |  Focus solely on gist token-based compression; exclusion of other context compression methods prevents a broader comparison and limits the conclusions. |\n| Proposal of two effective strategies (fine-grained autoencoding and segment-wise token importance estimation) to improve compression performance. |\n\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**\n\nSummary: Molar, a novel multimodal large language model framework, enhances sequential recommendation by integrating multiple content modalities with collaborative filtering signals through a post-alignment mechanism, significantly outperforming traditional and existing LLM-based methods.\n\n| Strengths | Weaknesses |\n|---|---|\n|  Effectively integrates multimodal data (text, images) with collaborative filtering signals for improved accuracy and robustness. | Requires multi-task fine-tuning, which can be computationally expensive and time-consuming. |\n|  Post-alignment mechanism prevents premature integration of collaborative filtering, preserving the strengths of both LLM and traditional methods. | Performance heavily depends on the underlying capabilities of the MLLM used; suboptimal base models can degrade overall performance. |\n| Consistently outperforms traditional and state-of-the-art LLM-based baselines across multiple datasets. | Unable to train larger LLMs due to computational constraints. |\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**\n\nSummary: MMFactory is a universal framework that acts as a solution search engine for vision-language tasks, suggesting multiple programmatic solutions based on user-defined tasks, sample input-output pairs, and constraints, and benchmarking their performance and resource characteristics.\n\n| Strengths | Weaknesses |\n|---|---|\n| Proposes multiple programmatic solutions for a given task, allowing users to choose the best option based on their constraints. | The framework relies on the availability of a suitable pool of pre-trained models, which might limit its applicability if such models are not accessible or insufficient for a specific task. |\n| Generates solutions applicable to all instances of a user-defined task, reducing the need for repeated solution generation for individual samples. | The computational cost of running the multi-agent system and generating solutions can be high, especially as the number of solutions in the pool increases. |\n| Leverages a multi-agent system for solution proposal, enhancing the robustness and quality of generated solutions.  |  The reliance on large language models as the core component makes the method dependent on the capabilities and biases of these models. |\n\n\n\n---\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified HTML printing\n",
        "page = f\"\"\"<html>\n",
        "<head>\n",
        "    <style>\n",
        "        table {{\n",
        "            border-collapse: collapse;\n",
        "            width: 100%;\n",
        "            margin: 20px 0;\n",
        "        }}\n",
        "        th, td {{\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 8px;\n",
        "            text-align: left;\n",
        "        }}\n",
        "        th {{\n",
        "            background-color: #f2f2f2;\n",
        "        }}\n",
        "    </style>\n",
        "    <h1>Daily Dose of AI Research</h1>\n",
        "    <h4>{date.today()}</h4>\n",
        "    <p><i>Analysis generated with: {LLM}</i></p>\n",
        "</head>\n",
        "<body>\"\"\"\n",
        "\n",
        "with open(\"papers_table.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "\n",
        "\n",
        "for paper in papers:\n",
        "    analysis_html = paper['analysis'].replace('|', '</td><td>').replace('\\n', '</td></tr><tr><td>')\n",
        "    page = f\"\"\"\n",
        "    <h2><a href=\"{paper['url']}\">{paper['title']}</a></h2>\n",
        "    <table>\n",
        "        <tr><td>{analysis_html}</td></tr>\n",
        "    </table>\n",
        "    <hr>\"\"\"\n",
        "    with open(\"papers_table.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "\n",
        "\n",
        "end = \"</body></html>\"\n",
        "with open(\"papers_table.html\", \"a\") as f:\n",
        "    f.write(end)"
      ],
      "metadata": {
        "id": "iTjRNznKdqHn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open source model setup\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\", # Use GPU if available\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500, # Reduced max tokens\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.1, # Increased temperature slightly\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9, # Added top_p sampling\n",
        "}\n",
        "\n",
        "# Modified paper analysis loop for the open source model\n",
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        # Limit the input text length\n",
        "        pdf_text = extract_pdf(paper[\"url\"])\n",
        "        max_input_length = 2000\n",
        "        truncated_text = pdf_text[:max_input_length]\n",
        "\n",
        "        messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a research paper analyzer. Provide analysis in a table format with strengths and weaknesses.\"\n",
        "        }, {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Analyze this research article and provide:\n",
        "1. A brief one-sentence summary\n",
        "2. Key strengths (list 2-3 points)\n",
        "3. Key weaknesses (list 2-3 points)\n",
        "\n",
        "Format the response as follows:\n",
        "Summary: [one sentence]\n",
        "| Strengths | Weaknesses |\n",
        "| --- | --- |\n",
        "| [strength 1] | [weakness 1] |\n",
        "| [strength 2] | [weakness 2] |\n",
        "| [strength 3] | [weakness 3] |\n",
        "\n",
        "Article text: {truncated_text}\"\"\"\n",
        "        }]\n",
        "\n",
        "        paper[\"analysis\"] = pipe(messages, **generation_args)[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Generation failed for {paper['title']}: {e}\")\n",
        "        paper[\"analysis\"] = \"Paper not available\"\n",
        "\n",
        "# Modified markdown printing\n",
        "for paper in papers:\n",
        "    printmd(f\"\"\"**[{paper['title']}]({paper['url']})**\\n\n",
        "{paper['analysis']}\\n\\n---\\n\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a7f9ae77fa234da9804cd59ef4af210e",
            "7d2aef8ff8f7453ba14f87cd83c72005",
            "2ad33796e7c344b987c6bdfd16ec32a2",
            "960c4958858548048597b3b9bcb7cdac",
            "369f29c1779447cd892dd5fa5f798d5d",
            "9de98632e4a249b2a0964c139448a8e4",
            "fc2e4ca3ed73436b810a8a0ee1aa5334",
            "edc9cac9ab7a4bc8be6e1b5a9478185c",
            "2d72a2fc4ecf40e48624afab93d87d76",
            "e8978f316657416d836db98c9a998b6f",
            "743eee8b7ced411ab16f044c2d077639"
          ]
        },
        "id": "vc2Afls-qnB8",
        "outputId": "7ffbe012-7a9b-4e36-c256-b67b1ae8c8ed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7f9ae77fa234da9804cd59ef4af210e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "100%|██████████| 4/4 [01:59<00:00, 29.97s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**\n\n Summary: YuLan-Mini is a highly efficient language model with 2.42B parameters that delivers top-tier performance with a significantly reduced data requirement compared to industry-leading models, achieved through an innovative pre-training approach.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Achieves top-tier performance with a significantly smaller dataset (1.08T tokens) compared to industry standards, demonstrating data efficiency. | 1. The paper may not fully address the potential limitations or challenges in scaling the model beyond the current parameter size or data volume. |\n| 2. Introduces a novel pre-training approach with three key technical contributions (data pipeline, robust optimization, and effective annealing), which could be beneficial for future research and development in the field. | 2. The paper's detailed technical report might be complex and require a deep understanding of machine learning and language modeling, potentially limiting accessibility for a broader audience. |\n| 3. Facilitates reproducibility and further research by releasing full details of the data composition for each training phase and project details on GitHub, promoting transparency and collaboration in the AI community. | 3. The paper does not discuss the potential ethical considerations or biases that may arise from the model's performance, which is an important aspect of responsible AI development and deployment. |\n\nNote: The weaknesses listed are inferred based on common challenges and considerations in AI research and development. The actual weaknesses may vary depending on the specific context and depth of the paper's analysis.\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**\n\n Summary: The study investigates gist-based context compression methods to enhance long-context processing in large language models, revealing near-lossless performance in certain tasks but identifying challenges and failure patterns in others, and proposing strategies to mitigate these issues.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Comprehensive investigation of gist-based context compression methods, providing valuable insights into their effectiveness. | 1. Identified specific tasks (e.g., synthetic recall) where gist-based compression faces challenges, indicating limitations in its applicability. |\n| 2. Proposed practical strategies (fine-grained autoencoding and segment-wise token importance estimation) to improve compression capabilities and address identified issues. | 2. The study's findings are based on extensive experiments, but the generalizability of the results to other models or tasks may not be fully established. |\n| 3. Contribution to the broader field of artificial intelligence by advancing understanding of long-context processing in large language models, which is crucial for future AI development. | 3. The study focuses on gist-based compression, which may not encompass all possible context compression techniques, potentially limiting the scope of future research in this area. |\n\nNote: The strengths and weaknesses provided are inferred from the given abstract and may not cover all aspects of the full research article. A more detailed analysis would require access to the complete text.\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**\n\n Summary: Molar is a novel framework for sequential recommendation that integrates multimodal content with collaborative filtering signals to enhance recommendation accuracy.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. **Integration of Multimodal Data**: Molar effectively combines textual and non-textual data, enriching item embeddings and capturing a more comprehensive representation of items. | 1. **Complexity and Resource Intensity**: The framework may require significant computational resources due to the integration of multiple modalities and the use of large language models, potentially limiting its scalability or accessibility. |\n| 2. **Collaborative Filtering Alignment**: By aligning user representations from content-based and ID-based models, Molar ensures precise personalization, leading to robust performance in recommendation tasks. | 2. **Data Privacy Concerns**: The use of ID information for collaborative filtering could raise privacy concerns, especially if not handled with proper anonymization and security measures. |\n| 3. **Superior Performance**: Extensive experiments demonstrate that Molar significantly outperforms traditional and LLM-based baselines, indicating its effectiveness in leveraging multimodal data and collaborative signals for sequential recommendation. | 3. **Generalization Across Diverse Domains**: While Molar shows strong performance, its effectiveness may vary across different domains or datasets, requiring further investigation to ensure its adaptability and generalizability. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**\n\n Summary: MMFactory is a universal framework that acts as a solution search engine for vision-language tasks, offering a diverse pool of programmatic solutions based on task descriptions, sample inputs/outputs, and user-defined constraints.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Provides a universal framework that can handle a variety of vision-language tasks, offering flexibility and accessibility. | 1. The effectiveness of the suggested solutions may depend on the quality and relevance of the input-output pairs provided, which could limit the framework's applicability in some scenarios. |\n| 2. Incorporates user-defined constraints such as performance and resource requirements, allowing for more tailored and efficient solutions. | 2. The framework's performance and accuracy might be influenced by the quality and diversity of the models available in its repository, which could limit its effectiveness if the repository is not comprehensive. |\n| 3. Utilizes a committee-based solution proposer and leverages multi-agent LLM conversation, potentially enhancing the generation of diverse, universal, and robust solutions. | 3. The complexity of the framework and its reliance on advanced AI components might pose challenges for users with limited technical expertise, potentially limiting its accessibility and ease of use. |\n| 4. Offers metrics and benchmarks for performance and resource characteristics, enabling users to make informed decisions based on their unique design constraints. | 4. The framework's ability to synthesize solutions and propose metrics might require significant computational resources, which could be a barrier for users with limited access to such resources. |\n| 5. Acts as a solution search engine, streamlining the process of finding suitable models and tools for vision-language tasks, which could save time and effort for users. | 5. The framework's reliance on a model repository means that its effectiveness is contingent on the continuous updating and maintenance of this repository, which could be a potential limitation. |\n\n---\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7f9ae77fa234da9804cd59ef4af210e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d2aef8ff8f7453ba14f87cd83c72005",
              "IPY_MODEL_2ad33796e7c344b987c6bdfd16ec32a2",
              "IPY_MODEL_960c4958858548048597b3b9bcb7cdac"
            ],
            "layout": "IPY_MODEL_369f29c1779447cd892dd5fa5f798d5d"
          }
        },
        "7d2aef8ff8f7453ba14f87cd83c72005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9de98632e4a249b2a0964c139448a8e4",
            "placeholder": "​",
            "style": "IPY_MODEL_fc2e4ca3ed73436b810a8a0ee1aa5334",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "2ad33796e7c344b987c6bdfd16ec32a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edc9cac9ab7a4bc8be6e1b5a9478185c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d72a2fc4ecf40e48624afab93d87d76",
            "value": 2
          }
        },
        "960c4958858548048597b3b9bcb7cdac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8978f316657416d836db98c9a998b6f",
            "placeholder": "​",
            "style": "IPY_MODEL_743eee8b7ced411ab16f044c2d077639",
            "value": " 2/2 [00:21&lt;00:00, 10.61s/it]"
          }
        },
        "369f29c1779447cd892dd5fa5f798d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de98632e4a249b2a0964c139448a8e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc2e4ca3ed73436b810a8a0ee1aa5334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edc9cac9ab7a4bc8be6e1b5a9478185c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d72a2fc4ecf40e48624afab93d87d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8978f316657416d836db98c9a998b6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743eee8b7ced411ab16f044c2d077639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}