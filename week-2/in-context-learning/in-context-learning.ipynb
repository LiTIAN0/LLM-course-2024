{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDSSSkaaDHQ"
      },
      "source": [
        "# In-Context Learning\n",
        "\n",
        "\n",
        "In-context learning is a generalisation of few-shot learning where the LLM is provided a context as part of the prompt and asked to respond by utilising the information in the context.\n",
        "\n",
        "* Example: *\"Summarize this research article into one paragraph highlighting its strengths and weaknesses: [insert article text]”*\n",
        "* Example: *\"Extract all the quotes from this text and organize them in alphabetical order: [insert text]”*\n",
        "\n",
        "A very popular technique that you will learn in week 5 called Retrieval-Augmented Generation (RAG) is a form of in-context learning, where:\n",
        "* a search engine is used to retrieve some relevant information\n",
        "* that information is then provided to the LLM as context\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7dVVH85aDHR"
      },
      "source": [
        "In this example we download some recent research papers from arXiv papers, extract the text from the PDF files and ask Gemini to summarize the articles as well as provide the main strengths and weaknesses of the papers. Finally we print the summaries to a local html file and as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vjT6123RaDHS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from urllib.request import urlopen, urlretrieve\n",
        "from IPython.display import Markdown, display\n",
        "from pypdf import PdfReader\n",
        "from datetime import date\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ixOGTDJ-aDHT"
      },
      "outputs": [],
      "source": [
        "#API_KEY = os.environ.get(\"GEMINI_API_KEY\")\n",
        "API_KEY = \"\"\n",
        "genai.configure(api_key=API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWktXpbZaDHT"
      },
      "source": [
        "We select those papers that have been featured in Hugging Face papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "kireq1dYaDHT"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"https://huggingface.co/papers\"\n",
        "page = requests.get(BASE_URL)\n",
        "soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "h3s = soup.find_all(\"h3\")\n",
        "\n",
        "papers = []\n",
        "\n",
        "for h3 in h3s:\n",
        "    a = h3.find(\"a\")\n",
        "    title = a.text\n",
        "    link = a[\"href\"].replace('/papers', '')\n",
        "\n",
        "    papers.append({\"title\": title, \"url\": f\"https://arxiv.org/pdf{link}\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HImBOlCfaDHT"
      },
      "source": [
        "Code to extract text from PDFs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nREjMfs6aDHU"
      },
      "outputs": [],
      "source": [
        "def extract_paper(url):\n",
        "    html = urlopen(url).read()\n",
        "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "\n",
        "    # kill all script and style elements\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()    # rip it out\n",
        "\n",
        "    # get text\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # break into lines and remove leading and trailing space on each\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    # break multi-headlines into a line each\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    # drop blank lines\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_pdf(url):\n",
        "    pdf = urlretrieve(url, \"pdf_file.pdf\")\n",
        "    reader = PdfReader(\"pdf_file.pdf\")\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "18EVEyyIaDHU"
      },
      "outputs": [],
      "source": [
        "LLM = \"gemini-1.5-flash\"\n",
        "model = genai.GenerativeModel(LLM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_He2wZSaaDHU"
      },
      "source": [
        "We use Gemini to summarize the papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OPOc0zVRaDHU",
        "outputId": "937810c7-cc49-42b1-f262-668cea1e91cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:19<00:00,  4.84s/it]\n"
          ]
        }
      ],
      "source": [
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        paper[\"summary\"] = model.generate_content(\"Summarize this research article into one paragraph without formatting highlighting its strengths and weaknesses. \" + extract_pdf(paper[\"url\"])).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"summary\"] = \"Paper not available\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kHeWIpBaDHV"
      },
      "source": [
        "We print the results to a html file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "SFMaW_aeaDHV"
      },
      "outputs": [],
      "source": [
        "page = f\"<html> <head> <h1>Daily Dose of AI Research</h1> <h4>{date.today()}</h4> <p><i>Summaries generated with: {LLM}</i>\"\n",
        "with open(\"papers.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "for paper in papers:\n",
        "    page = f'<h2><a href=\"{paper[\"url\"]}\">{paper[\"title\"]}</a></h2> <p>{paper[\"summary\"]}</p>'\n",
        "    with open(\"papers.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "end = \"</head>  </html>\"\n",
        "with open(\"papers.html\", \"a\") as f:\n",
        "    f.write(end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_PmBc5naDHV"
      },
      "source": [
        "We can also print the results to this notebook as markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "9YECtrV6aDHV",
        "outputId": "5f0823fc-5273-4313-8c31-84a7ea34eddf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**<br>This research paper details the development of YuLan-Mini, a 2.42B parameter language model achieving top-tier performance among similarly sized models.  Its strengths lie in a meticulously designed data pipeline combining data cleaning, scheduling strategies, and synthetic data generation, particularly for reasoning tasks.  A robust optimization method, incorporating techniques like µP initialization and WeSaR re-parameterization, effectively mitigates training instability.  Furthermore, an annealing approach with targeted data selection and long context training further boosts performance.  However, a weakness is the reliance on a relatively small training dataset (1.08T tokens) compared to industry-leading models, limiting its long-context capabilities.  While the open-sourcing of the model and detailed training procedures is a significant contribution, the limited computational resources available may hinder complete reproducibility for other researchers.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**<br>This research paper comprehensively investigates gist token-based context compression methods for improving long-context processing in large language models (LLMs).  The study finds that while these methods achieve near-lossless performance on tasks like retrieval-augmented generation and long-document QA, they struggle with tasks requiring precise recall, exhibiting three key failure patterns: information loss at segment boundaries, loss of surprising details, and loss of information mid-sequence.  Strengths include a unified framework for categorizing existing methods, extensive experimentation across diverse tasks, and the proposal of two effective mitigation strategies: fine-grained autoencoding and segment-wise token importance estimation.  However, a weakness is the limitation to relatively small LLMs (7-8B parameters) and a focus solely on gist token-based compression, potentially overlooking other effective context compression techniques.  Further research on larger models and a broader comparison of methods is needed.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**<br>Molar is a novel sequential recommendation framework that integrates multimodal large language models (MLLMs) with collaborative filtering.  It addresses the limitations of existing LLM-based methods which neglect collaborative filtering information by using an MLLM to generate item representations from textual and non-textual data, and then aligning user representations from content-based and ID-based models via a post-alignment mechanism.  Experiments demonstrate Molar's superior performance compared to traditional and LLM-based baselines across multiple datasets.  However, a key weakness is the computationally intensive multi-task fine-tuning process, hindering real-time applications.  Furthermore, performance is dependent on the underlying capabilities of the MLLM used, limiting scalability to larger models.\n<br><br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**<br>This research introduces MMFactory, a novel framework for solving vision-language tasks that acts as a solution search engine.  Unlike previous approaches that rely on single models or sample-specific solutions, MMFactory proposes a diverse pool of programmatic solutions by combining various vision, language, and vision-language models based on a user-provided task description, sample input-output pairs, and optional constraints (e.g., computational resources).  A committee-based multi-agent LLM system generates these executable solutions, ensuring robustness and diversity.  The framework also includes a metric router to evaluate and benchmark the performance and resource usage of each solution, enabling users to select the optimal solution for their needs.  While experimental results demonstrate state-of-the-art performance on benchmark datasets, a potential weakness is the computational cost of the multi-agent system, although this is mitigated by the reusability of generated solutions across all task instances.  Furthermore, the reliance on LLMs for both solution and metric routing might introduce biases and limitations inherent to these models.\n<br><br>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for paper in papers:\n",
        "    printmd(\"**[{}]({})**<br>{}<br><br>\".format(paper[\"title\"],\n",
        "                                                paper[\"url\"],\n",
        "                                                paper[\"summary\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified prompt for tabulated analysis\n",
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        prompt = \"\"\"Analyze this research article and provide:\n",
        "1. A brief one-sentence summary\n",
        "2. Key strengths (list 3 points)\n",
        "3. Key weaknesses (list 3 points)\n",
        "\n",
        "Format the response as follows:\n",
        "Summary: [one sentence]\n",
        "| Strengths | Weaknesses |\n",
        "| --- | --- |\n",
        "| [strength 1] | [weakness 1] |\n",
        "| [strength 2] | [weakness 2] |\n",
        "| [strength 3] | [weakness 3] |\n",
        "\n",
        "Article text: \"\"\" + extract_pdf(paper[\"url\"])\n",
        "\n",
        "        paper[\"analysis\"] = model.generate_content(prompt).text\n",
        "    except:\n",
        "        print(\"Generation failed\")\n",
        "        paper[\"analysis\"] = \"Paper not available\"\n",
        "\n",
        "# Modified markdown printing\n",
        "for paper in papers:\n",
        "    printmd(f\"\"\"**[{paper['title']}]({paper['url']})**\\n\n",
        "{paper['analysis']}\\n\\n---\\n\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wbimiUyMcVmB",
        "outputId": "7398aadb-c04e-4121-fa79-3f1928dab4ae"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:21<00:00,  5.42s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**\n\nSummary: YuLan-Mini is a data-efficient 2.42B parameter language model achieving top-tier performance among similarly sized models by employing an elaborate data pipeline, a robust optimization method, and an effective annealing approach.\n\n| Strengths | Weaknesses |\n|---|---|\n| Achieves top-tier performance comparable to much larger models, demonstrating high data efficiency. | Limited long-context capability due to resource constraints; only achieved 28K context window. |\n| Open-source and reproducible; full training details and data composition are released. |  The study primarily focuses on a specific model and its training methods; generalization to other models is not fully explored. |\n|  Employs a comprehensive approach to training stability, combining several methods to mitigate instability issues. | The reproducibility of the baseline models' results is challenged due to incomplete reporting of evaluation setup. |\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**\n\nSummary: This research comprehensively investigates gist token-based context compression for large language models, revealing its effectiveness in many tasks but identifying key failure patterns and proposing strategies to mitigate them.\n\n| Strengths | Weaknesses |\n|---|---|\n| Comprehensive evaluation across diverse language tasks, including language modeling, weak context-dependent tasks, and long context tasks.  | Limited model size and context length explored due to computational resource constraints;  larger models might show different results. |\n| Identification of three critical failure patterns (lost by the boundary, lost if surprise, lost along the way) arising from compression bottlenecks, providing valuable insights into the limitations of the method. | Focus solely on gist token-based compression;  other context compression methods are not included in the comparative analysis.  |\n| Proposal of two effective strategies (fine-grained autoencoding and segment-wise token importance estimation) to mitigate identified weaknesses and improve model performance. |  While the proposed mitigation strategies show improvement, they don't entirely eliminate the identified failure patterns. |\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**\n\nSummary: Molar, a novel multimodal large language model framework, enhances sequential recommendation by integrating multiple content modalities with collaborative filtering signals through a post-alignment mechanism, achieving superior performance compared to existing methods.\n\n| Strengths | Weaknesses |\n|---|---|\n|  Combines multimodal data (text, images) with collaborative filtering, leveraging the strengths of both approaches. | Requires multi-task fine-tuning, which can be computationally expensive and time-consuming. |\n|  Utilizes a post-alignment mechanism to effectively integrate collaborative filtering signals without hindering the LLM's semantic understanding. | Performance heavily relies on the underlying capabilities of the MLLM; suboptimal base models can degrade overall performance. |\n|  Consistently outperforms traditional and LLM-based baselines across multiple datasets, demonstrating its effectiveness and robustness. |  Limited by computational constraints; inability to train larger LLMs may hinder further performance improvements. |\n\n\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**\n\nSummary: MMFactory is a universal framework that acts as a solution search engine for vision-language tasks, suggesting diverse programmatic solutions tailored to user specifications and constraints by combining various models.\n\n| Strengths | Weaknesses |\n|---|---|\n| Proposes multiple programmatic solutions for a given task, allowing users to choose based on performance and resource constraints. |  The reliance on a large language model (LLM) like GPT-4 as the core component raises concerns about cost and accessibility for users. |\n|  Addresses limitations of existing methods by considering user constraints (computation, performance) and generating generalized solutions applicable to all task instances, not just individual examples. | The paper lacks detailed information on the size and complexity of the MMFactory framework itself, making it difficult to assess its scalability and practical deployment. |\n| Outperforms state-of-the-art methods on two benchmarks (BLINK and Seedbench) by delivering tailored solutions.  | The ablation study, while informative, is limited in scope, focusing primarily on the individual components of the multi-agent system rather than a broader evaluation of the framework's overall performance. |\n\n\n\n---\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified HTML printing\n",
        "page = f\"\"\"<html>\n",
        "<head>\n",
        "    <style>\n",
        "        table {{\n",
        "            border-collapse: collapse;\n",
        "            width: 100%;\n",
        "            margin: 20px 0;\n",
        "        }}\n",
        "        th, td {{\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 8px;\n",
        "            text-align: left;\n",
        "        }}\n",
        "        th {{\n",
        "            background-color: #f2f2f2;\n",
        "        }}\n",
        "    </style>\n",
        "    <h1>Daily Dose of AI Research</h1>\n",
        "    <h4>{date.today()}</h4>\n",
        "    <p><i>Analysis generated with: {LLM}</i></p>\n",
        "</head>\n",
        "<body>\"\"\"\n",
        "\n",
        "with open(\"papers_table.html\", \"w\") as f:\n",
        "    f.write(page)\n",
        "\n",
        "\n",
        "for paper in papers:\n",
        "    analysis_html = paper['analysis'].replace('|', '</td><td>').replace('\\n', '</td></tr><tr><td>')\n",
        "    page = f\"\"\"\n",
        "    <h2><a href=\"{paper['url']}\">{paper['title']}</a></h2>\n",
        "    <table>\n",
        "        <tr><td>{analysis_html}</td></tr>\n",
        "    </table>\n",
        "    <hr>\"\"\"\n",
        "    with open(\"papers_table.html\", \"a\") as f:\n",
        "        f.write(page)\n",
        "\n",
        "\n",
        "end = \"</body></html>\"\n",
        "with open(\"papers_table.html\", \"a\") as f:\n",
        "    f.write(end)"
      ],
      "metadata": {
        "id": "iTjRNznKdqHn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open source model setup\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\", # Use GPU if available\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500, # Reduced max tokens\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.1, # Increased temperature slightly\n",
        "    \"do_sample\": True,\n",
        "    \"top_p\": 0.9, # Added top_p sampling\n",
        "}\n",
        "\n",
        "# Modified paper analysis loop for the open source model\n",
        "for paper in tqdm(papers):\n",
        "    try:\n",
        "        # Limit the input text length\n",
        "        pdf_text = extract_pdf(paper[\"url\"])\n",
        "        max_input_length = 2000\n",
        "        truncated_text = pdf_text[:max_input_length]\n",
        "\n",
        "        messages = [{\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a research paper analyzer. Provide analysis in a table format with strengths and weaknesses.\"\n",
        "        }, {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Analyze this research article and provide:\n",
        "1. A brief one-sentence summary\n",
        "2. Key strengths (list 3 points)\n",
        "3. Key weaknesses (list 3 points)\n",
        "\n",
        "Format the response as follows:\n",
        "Summary: [one sentence]\n",
        "| Strengths | Weaknesses |\n",
        "| --- | --- |\n",
        "| [strength 1] | [weakness 1] |\n",
        "| [strength 2] | [weakness 2] |\n",
        "| [strength 3] | [weakness 3] |\n",
        "\n",
        "Article text: {truncated_text}\"\"\"\n",
        "        }]\n",
        "\n",
        "        paper[\"analysis\"] = pipe(messages, **generation_args)[0]['generated_text']\n",
        "    except Exception as e:\n",
        "        print(f\"Generation failed for {paper['title']}: {e}\")\n",
        "        paper[\"analysis\"] = \"Paper not available\"\n",
        "\n",
        "# Modified markdown printing\n",
        "for paper in papers:\n",
        "    printmd(f\"\"\"**[{paper['title']}]({paper['url']})**\\n\n",
        "{paper['analysis']}\\n\\n---\\n\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9acd0fbefdc4e3c9507652c5b8e4a25",
            "fc4c37646cc542d5a2c79b98addbffcd",
            "8965f20f319246fb8c57414a431bdb16",
            "3f456662f19342788b26cd328f20824f",
            "31d1fa86b9b1498c9019a389b06f97b1",
            "6f0a11dd676748a18ad043e4a5290e77",
            "ae2e8e12aa844f578d5dc7a79a81a054",
            "b3d4b219593942e5b016dc34de5ac4a9",
            "c75bb01103254bbeaf2bc422e6e3a46c",
            "0a0278a9df694d85a2edf81e56ab7eea",
            "2c7013604b27476488babf0d072fda00"
          ]
        },
        "id": "vc2Afls-qnB8",
        "outputId": "80ca9aee-e0ee-4f8a-b2cb-34a3095122ef"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9acd0fbefdc4e3c9507652c5b8e4a25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "100%|██████████| 4/4 [02:12<00:00, 33.17s/it]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[YuLan-Mini: An Open Data-efficient Language Model](https://arxiv.org/pdf/2412.17743)**\n\n Summary: YuLan-Mini is a highly efficient language model with 2.42B parameters that achieves top performance with significantly less data than industry-leading models, thanks to an innovative pre-training approach.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Achieves top-tier performance with a significantly smaller dataset (1.08T tokens) compared to industry standards, demonstrating data efficiency. | 1. The paper may not fully address the potential limitations or challenges in scaling the model beyond the current parameter size. |\n| 2. Introduces a novel pre-training approach with three key technical contributions (data pipeline, robust optimization, and effective annealing), which could be beneficial for future research and development in the field. | 2. The effectiveness of the proposed techniques may be context-dependent, and further studies are needed to evaluate their generalizability across different tasks and domains. |\n| 3. Facilitates reproducibility and further research by releasing full details of the data composition for each training phase and providing access to project details on GitHub. | 3. The paper does not discuss the computational resources required for training YuLan-Mini, which could be a barrier for researchers with limited access to high-performance computing infrastructure. |\n\nNote: The weaknesses listed are hypothetical and based on common challenges in research papers. The actual weaknesses would depend on a thorough analysis of the paper's content, methodology, and results.\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[A Silver Bullet or a Compromise for Full Attention? A Comprehensive Study of Gist Token-based Context Compression](https://arxiv.org/pdf/2412.17483)**\n\n Summary: The study investigates gist-based context compression methods to enhance long-context processing in large language models, revealing near-lossless performance in certain tasks but challenges in others, and proposing strategies to mitigate identified failure patterns.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| [Strength 1] Comprehensive investigation of gist-based context compression methods, providing valuable insights into their application and limitations. | [Weakness 1] The study may not cover all possible failure patterns or scenarios, leaving room for further exploration. |\n| [Strength 2] Identification of three key failure patterns (lost by the boundary, lost if surprise, and lost along the way), which helps in understanding the limitations of gist-based compression. | [Weakness 2] The proposed strategies (fine-grained autoencoding and segment-wise token importance estimation) may require significant computational resources, limiting their practicality in resource-constrained environments. |\n| [Strength 3] Practical strategies for improving compression capabilities, such as fine-grained autoencoding and segment-wise token importance estimation, offer actionable solutions for enhancing the performance of gist-based context compression. | [Weakness 3] The study's experimental results are focused on specific tasks (retrieval-augmented generation and long-document QA), which may not generalize to all types of long-context processing applications, potentially limiting the broader applicability of the findings. |\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[Molar: Multimodal LLMs with Collaborative Filtering Alignment for Enhanced Sequential Recommendation](https://arxiv.org/pdf/2412.18176)**\n\n Summary: Molar is a novel framework for sequential recommendation that integrates multimodal content with collaborative filtering signals, significantly improving recommendation accuracy over traditional and LLM-based methods.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. **Integration of Multimodal Data**: Molar effectively combines textual and non-textual data, enriching item representations and capturing a more comprehensive user interest profile. | 1. **Complexity and Resource Intensity**: The framework may require significant computational resources and expertise to implement, potentially limiting its accessibility and scalability. |\n| 2. **Enhanced Personalization**: By aligning user representations from content-based and ID-based models, Molar ensures precise personalization, leading to more relevant recommendations. | 2. **Dependency on Quality of Data**: The performance of Molar heavily relies on the quality and diversity of the multimodal data and collaborative filtering signals, which may not always be available or accurately maintained. |\n| 3. **Superior Performance in Experiments**: Extensive experimental validation demonstrates Molar's significant outperformance over traditional and LLM-based baselines, showcasing its effectiveness in capturing both user interests and contextual semantics. | 3. **Potential Overfitting**: The sophisticated modeling approach, while beneficial for performance, may lead to overfitting, especially in scenarios with limited or noisy data, affecting the model's generalizability. |\n\nThese strengths and weaknesses highlight Molar's innovative approach to sequential recommendation, balancing the benefits of multimodal data integration and collaborative filtering with considerations regarding implementation complexity, data dependency, and model robustness.\n\n---\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**[MMFactory: A Universal Solution Search Engine for Vision-Language Tasks](https://arxiv.org/pdf/2412.18072)**\n\n Summary: MMFactory is a universal framework that acts as a solution search engine for vision-language tasks, offering a diverse pool of programmatic solutions based on task descriptions, sample inputs/outputs, and user-defined constraints.\n\n| Strengths | Weaknesses |\n| --- | --- |\n| 1. Provides a universal framework that can handle a wide range of vision-language tasks, making it versatile and adaptable to various applications. | 1. The effectiveness of the suggested solutions may depend heavily on the quality and relevance of the input-output pairs provided, which could limit its applicability in scenarios with limited or ambiguous data. |\n| 2. Incorporates user-defined constraints such as resource and performance limitations, allowing for more tailored and efficient solutions. | 2. The framework's reliance on a model repository may restrict its ability to leverage the latest or most specialized models not included in the repository, potentially limiting its performance on cutting-edge tasks. |\n| 3. Utilizes a committee-based solution proposer and leverages multi-agent LLM conversation, enhancing the generation of diverse, universal, and robust solutions. | 3. The complexity of the framework and its reliance on advanced AI components may pose challenges in terms of usability for non-expert users, requiring a steeper learning curve or technical support. |\n| 4. Offers a systematic approach to synthesizing solutions by instantiating and combining various visio-lingual tools, potentially improving the efficiency of finding suitable solutions. | 4. The performance and resource characteristics proposed by the framework may not always accurately reflect real-world scenarios, leading to potential mismatches between expected and actual performance. |\n| 5. By proposing metrics and benchmarks, MMFactory empowers users to make informed decisions based on their unique design constraints, enhancing the overall usability and effectiveness of the framework. | 5. The framework's ability to generate executable solutions may be limited by the quality and compatibility of the underlying models and tools, potentially requiring additional integration efforts for optimal results. |\n\nNote: The strengths and weaknesses listed above are inferred from the provided article text and general knowledge of similar frameworks. The actual strengths and weaknesses may vary based on\n\n---\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9acd0fbefdc4e3c9507652c5b8e4a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc4c37646cc542d5a2c79b98addbffcd",
              "IPY_MODEL_8965f20f319246fb8c57414a431bdb16",
              "IPY_MODEL_3f456662f19342788b26cd328f20824f"
            ],
            "layout": "IPY_MODEL_31d1fa86b9b1498c9019a389b06f97b1"
          }
        },
        "fc4c37646cc542d5a2c79b98addbffcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f0a11dd676748a18ad043e4a5290e77",
            "placeholder": "​",
            "style": "IPY_MODEL_ae2e8e12aa844f578d5dc7a79a81a054",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8965f20f319246fb8c57414a431bdb16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3d4b219593942e5b016dc34de5ac4a9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c75bb01103254bbeaf2bc422e6e3a46c",
            "value": 2
          }
        },
        "3f456662f19342788b26cd328f20824f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a0278a9df694d85a2edf81e56ab7eea",
            "placeholder": "​",
            "style": "IPY_MODEL_2c7013604b27476488babf0d072fda00",
            "value": " 2/2 [00:21&lt;00:00, 10.72s/it]"
          }
        },
        "31d1fa86b9b1498c9019a389b06f97b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f0a11dd676748a18ad043e4a5290e77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2e8e12aa844f578d5dc7a79a81a054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b3d4b219593942e5b016dc34de5ac4a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c75bb01103254bbeaf2bc422e6e3a46c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a0278a9df694d85a2edf81e56ab7eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c7013604b27476488babf0d072fda00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}